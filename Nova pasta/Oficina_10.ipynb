{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementação de Word2Vec\n",
        "Segue abaixo um exemplo trivial de Word2Vec na prática. Vale observar que o modelo não tão preciso por causa da carência de dados de treino. Mas como o objetivo é apenas fazer uma demonstração, isso não será relevante."
      ],
      "metadata": {
        "id": "Qd2s0XvOxy13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------\n",
        "# Baixando e importando as dependencias\n",
        "# ------------------------------------\n",
        "!pip install gensim nltk\n",
        "\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "id": "SUZJQ6IAw9xM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------\n",
        "# Dados de Treinamento\n",
        "# --------------------------------------\n",
        "sentences = [\n",
        "    # Intenção: redefinir_senha\n",
        "    \"como redefinir minha senha\",\n",
        "    \"esqueci minha senha preciso de ajuda\",\n",
        "    \"quero alterar minha senha\",\n",
        "    \"preciso mudar minha senha\",\n",
        "    \"como trocar a senha do meu acesso\",\n",
        "\n",
        "    # Intenção: rastrear_pedido\n",
        "    \"como rastrear meu pedido\",\n",
        "    \"onde está meu pacote\",\n",
        "    \"status do meu pedido\",\n",
        "    \"quero saber onde está minha encomenda\",\n",
        "    \"como acompanhar a entrega do meu produto\",\n",
        "\n",
        "    # Intenção: cancelar_conta\n",
        "    \"quero cancelar minha assinatura\",\n",
        "    \"como encerrar minha conta\",\n",
        "    \"desejo deletar meu perfil\",\n",
        "    \"como posso excluir minha conta\",\n",
        "    \"preciso encerrar meu cadastro\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "WMYxGW9QxErQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------\n",
        "# Pré-processamento\n",
        "# --------------------------------------\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "additional_stopwords = [\"como\", \"quero\", \"preciso\", \"desejo\"]  # Palavras específicas do domínio\n",
        "stopwords += additional_stopwords\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = nltk.word_tokenize(text.lower(), language='portuguese')\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stopwords]\n",
        "    return tokens\n",
        "\n",
        "processed_sentences = [preprocess(sentence) for sentence in sentences]\n"
      ],
      "metadata": {
        "id": "9aZUu0ZKxMqB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------\n",
        "# Treinamento do Modelo com Parâmetros Ajustados\n",
        "# --------------------------------------\n",
        "model = Word2Vec(\n",
        "    sentences=processed_sentences,\n",
        "    vector_size=150,  # Aumentado para capturar mais nuances\n",
        "    window=7,         # Janela maior para contexto amplo\n",
        "    min_count=1,\n",
        "    workers=4,\n",
        "    epochs=50         # Mais épocas de treinamento\n",
        ")\n"
      ],
      "metadata": {
        "id": "ThbNAHRvxTRy"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------\n",
        "# Mapeamento de Intenções com Palavras-Chave Aprimoradas\n",
        "# --------------------------------------\n",
        "intent_vectors = {\n",
        "    \"redefinir_senha\": np.mean([\n",
        "        model.wv[\"redefinir\"],\n",
        "        model.wv[\"senha\"],\n",
        "        model.wv[\"alterar\"],\n",
        "        model.wv[\"mudar\"]\n",
        "    ], axis=0),\n",
        "\n",
        "    \"rastrear_pedido\": np.mean([\n",
        "        model.wv[\"rastrear\"],\n",
        "        model.wv[\"pedido\"],\n",
        "        model.wv[\"pacote\"],\n",
        "        model.wv[\"encomenda\"],\n",
        "        model.wv[\"entrega\"]\n",
        "    ], axis=0),\n",
        "\n",
        "    \"cancelar_conta\": np.mean([\n",
        "        model.wv[\"cancelar\"],\n",
        "        model.wv[\"conta\"],\n",
        "        model.wv[\"excluir\"],\n",
        "        model.wv[\"encerrar\"]\n",
        "    ], axis=0)\n",
        "}"
      ],
      "metadata": {
        "id": "8PulB1bOxhMI"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------\n",
        "# Função de Predição com Limiar de Confiança\n",
        "# --------------------------------------\n",
        "def predict_intent(query):\n",
        "    tokens = preprocess(query)\n",
        "    if not tokens:\n",
        "        return \"Intenção não reconhecida\"\n",
        "\n",
        "    # Calcula vetor apenas com palavras conhecidas\n",
        "    valid_vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    if not valid_vectors:\n",
        "        return \"Intenção não reconhecida\"\n",
        "\n",
        "    input_vector = np.mean(valid_vectors, axis=0)\n",
        "\n",
        "    best_intent = None\n",
        "    best_similarity = -1\n",
        "\n",
        "    for intent, vec in intent_vectors.items():\n",
        "        similarity = cosine_similarity([input_vector], [vec])[0][0]\n",
        "        if similarity > best_similarity:\n",
        "            best_similarity = similarity\n",
        "            best_intent = intent\n",
        "\n",
        "    # Limiar de confiança (ajustável conforme necessidade)\n",
        "    if best_similarity < 0.4:  # Rejeita similaridades baixas\n",
        "        return \"Intenção não reconhecida\"\n",
        "\n",
        "    return best_intent"
      ],
      "metadata": {
        "id": "VXbiWBL4xim2"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------\n",
        "# Testes Atualizados\n",
        "# --------------------------------------\n",
        "test_queries = [\n",
        "    \"mudar minha senha\",\n",
        "    \"onde está minha encomenda\",\n",
        "    \"apagar conta\",\n",
        "    \"parar assinatura\",\n",
        "    \"como acompanho meu pacote\",\n",
        "    \"quero excluir meu cadastro\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"'{query}' → {predict_intent(query)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c540GAmcxnym",
        "outputId": "1c5293b5-b175-4061-8f73-b0fb3882dbea"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'mudar minha senha' → redefinir_senha\n",
            "'onde está minha encomenda' → rastrear_pedido\n",
            "'apagar conta' → cancelar_conta\n",
            "'parar assinatura' → Intenção não reconhecida\n",
            "'como acompanho meu pacote' → rastrear_pedido\n",
            "'quero excluir meu cadastro' → Intenção não reconhecida\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exemplo de implementação de RNN(LSTM)\n",
        "O objetivo simular que o Chatbot agora vai aplicar os conceitos vistos nessa oficina."
      ],
      "metadata": {
        "id": "Wcyy2yliyUeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow gensim numpy scikit-learn --quiet\n"
      ],
      "metadata": {
        "id": "DjeXDwLf9tGD"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Importar as bibliotecas e preparar os dados\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Exemplo de dados em português\n",
        "texts = [\n",
        "    \"Eu adoro este produto\",\n",
        "    \"Não gostei, achei caro e já fiu\",\n",
        "    \"Estou muito feliz com o meu pedido\",\n",
        "    \"Isso é um absurdo, que decepção\"\n",
        "]\n",
        "\n",
        "labels = [\"positivo\", \"negativo\", \"positivo\", \"negativo\"]\n",
        "\n",
        "# Convertendo textos para listas de palavras\n",
        "processed_texts = [simple_preprocess(text) for text in texts]"
      ],
      "metadata": {
        "id": "9-8euS_0BIHa"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Treinar o modelo Word2Vec\n",
        "\n",
        "word2vec_model = Word2Vec(sentences=processed_texts, vector_size=100, window=5, min_count=1, sg=0)\n",
        "word2vec_model.wv.save(\"word2vec.wordvectors\")\n",
        "vocabulary = word2vec_model.wv.key_to_index"
      ],
      "metadata": {
        "id": "kGLBM5oiBUzw"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Preparar os dados para o modelo LSTM\n",
        "\n",
        "# Mapear palavras para índices\n",
        "def texts_to_sequences(texts, word2vec_model):\n",
        "    sequences = []\n",
        "    for text in texts:\n",
        "        seq = [word2vec_model.wv.key_to_index[word] for word in text if word in word2vec_model.wv]\n",
        "        sequences.append(seq)\n",
        "    return sequences\n",
        "\n",
        "sequences = texts_to_sequences(processed_texts, word2vec_model)\n",
        "max_len = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# Codificar rótulos\n",
        "le = LabelEncoder()\n",
        "encoded_labels = le.fit_transform(labels)\n",
        "one_hot_labels = to_categorical(encoded_labels)"
      ],
      "metadata": {
        "id": "O8CNxDCrBXGQ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Dividir o conjunto de dados e construir o modelo LSTM\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, one_hot_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Construir o modelo LSTM\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(vocabulary), output_dim=100, weights=[word2vec_model.wv.vectors], input_length=max_len, trainable=False))\n",
        "model.add(LSTM(128, return_sequences=False))\n",
        "model.add(Dense(2, activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Treino do modelo\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=2, validation_split=0.1)\n",
        "\n",
        "# Avaliar o modelo\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Acurácia:\", accuracy)\n",
        "print(\"Função perca\", loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tYNYE8EBZqi",
        "outputId": "2b62be3d-c66d-4fa2-9859-7993daca01a3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.0000e+00 - loss: 0.6938 - val_accuracy: 1.0000 - val_loss: 0.6928\n",
            "Epoch 2/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - accuracy: 0.5000 - loss: 0.6921 - val_accuracy: 0.0000e+00 - val_loss: 0.6954\n",
            "Epoch 3/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 0.6904 - val_accuracy: 0.0000e+00 - val_loss: 0.6959\n",
            "Epoch 4/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 1.0000 - loss: 0.6887 - val_accuracy: 0.0000e+00 - val_loss: 0.6951\n",
            "Epoch 5/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 0.6869 - val_accuracy: 0.0000e+00 - val_loss: 0.6948\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 0.6927\n",
            "Acurácia: 1.0\n",
            "Função perca 0.6926566958427429\n"
          ]
        }
      ]
    }
  ]
}